{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imshow # For testing\n",
    "from functools import partial\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define path names here\n",
    "image_path_prefix = 'data/Category and Attribute Prediction Benchmark/Img/Img/'\n",
    "proc_img_path_prefix = 'data/Category and Attribute Prediction Benchmark/Img/Img/proc_'\n",
    "image_to_category = 'data/Category and Attribute Prediction Benchmark/Anno/list_category_img.txt'\n",
    "img_net_class_path = 'data/ImageNetClasses/imagenet1000_clsid_to_human.txt'\n",
    "deep_fash_class_path = 'data/Category and Attribute Prediction Benchmark/Anno/list_category_cloth.txt'\n",
    "proc_img_path = 'intermediates/processed_images.hdf5'\n",
    "\n",
    "n_images_loaded = -1 # -1 loads all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread # uses PIL on the backend\n",
    "\n",
    "def load_data(identifier,chunksize):\n",
    "    labels = determine_data_subset(identifier)\n",
    "    for image_info in read_data(labels,chunksize):\n",
    "        yield (len(labels) + chunksize - 1)//chunksize, image_info\n",
    "        \n",
    "def determine_data_subset(identifier):\n",
    "    n_top_labels = 10\n",
    "    \n",
    "    # Read in the attribute information to figure out what images to load\n",
    "    cat_labels = pd.read_csv(identifier,skiprows=1,sep='\\s+')\n",
    "    \n",
    "    # Figure out which are the most frequent labels to keep\n",
    "    cat_label_count = cat_labels.groupby('category_label').count()\n",
    "    cat_label_count = cat_label_count.sort_values(by=['image_name'],ascending=False)\n",
    "    kept_labels = cat_label_count.head(n_top_labels).index\n",
    "    \n",
    "    # Filter labels not in the n_top_labels most frequent\n",
    "    cat_labels = cat_labels.loc[cat_labels['category_label'].isin(kept_labels)]\n",
    "    \n",
    "    if n_images_loaded != -1:\n",
    "        return cat_labels.head(n_images_loaded)\n",
    "    else:\n",
    "        return cat_labels\n",
    "\n",
    "def read_data(labels,chunksize):\n",
    "    def load_image_data(image_path):\n",
    "        \"\"\"Given an image path, load the data as an array\"\"\"\n",
    "        image = imread(image_path_prefix+image_path,plugin='pil')\n",
    "        return image\n",
    "    \n",
    "    # Store arrays containing the image data for all images\n",
    "    for chunk in range(0,len(labels),chunksize):\n",
    "        label_subset = labels.iloc[chunk:chunk+chunksize,:]\n",
    "        label_subset['image'] = label_subset['image_name'].apply(load_image_data)\n",
    "        yield label_subset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data['image'] = scale_images(data['image'])\n",
    "    data['image'] = fill_image(data['image'])\n",
    "    return data\n",
    "\n",
    "def scale_images(image_series):\n",
    "    \"\"\"\n",
    "    Scale images so first dimension is 224 for input to ImageNet\n",
    "    \"\"\"\n",
    "    def first_dim_224(img):\n",
    "        \"\"\"Scale first dimension of image to 224 pixels\"\"\"\n",
    "        height = img.shape[0]\n",
    "        return rescale(img,224/height,mode='constant')\n",
    "    rescaled = image_series.apply(first_dim_224)\n",
    "    return rescaled\n",
    "\n",
    "def fill_image(image_series):\n",
    "    \"\"\"\n",
    "    Pad second dimension of images with black or crop until length 224 reached\n",
    "    \"\"\"\n",
    "    def pad_with_black(img):\n",
    "        \"\"\"\n",
    "        If image's x dim is less than 224 pixels pad with black. If too large crop to 224.\n",
    "        Modification spread evenly across both sides.\n",
    "        \"\"\"\n",
    "        pix_to_pad = 224 - img.shape[1]\n",
    "        if pix_to_pad < 0:\n",
    "            # Crop the image\n",
    "            pix_to_crop = -pix_to_pad\n",
    "            img = img[:,pix_to_crop//2:-((pix_to_crop+1)//2),:]\n",
    "        else:\n",
    "            # Pad the image\n",
    "            img = np.pad(img,[(0,0),((pix_to_pad+1)//2,pix_to_pad//2),(0,0)],mode='constant')\n",
    "        return img\n",
    "    padded = image_series.apply(pad_with_black)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave\n",
    "\n",
    "def save_proc_images(img_df):\n",
    "    \"\"\"Saves processed images to disk. Assumes directory structure already in place\"\"\"\n",
    "    def save_image(row):\n",
    "        imsave(proc_img_path_prefix+row['image_name'],row['image'],plugin='pil')\n",
    "    img_df.apply(save_image,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Save processed images to disk as jpeg\n",
    "chunksize = 5000\n",
    "\n",
    "start = perf_counter(); image_count = 0\n",
    "i_chunk = 0\n",
    "for n_chunk, img_df in load_data(image_to_category,chunksize):\n",
    "    img_df = preprocess_data(img_df)\n",
    "    save_proc_images(img_df)\n",
    "    \n",
    "    image_count += len(img_df)\n",
    "    i_chunk += 1    \n",
    "    print('Chunk {} of {} complete'.format(i_chunk,n_chunk))\n",
    "print('Took {} seconds to preprocess {} images'.format(perf_counter() - start,image_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle storing data to HDF5 here. Need special treatment due to numpy arrays\n",
    "def store_to_hdf5(hdfstore,img_df,i_chunk):\n",
    "    img_df[['image_name','category_label']].to_hdf(hdfstore,'chunk{}/info'.format(i_chunk))\n",
    "    \n",
    "    hdfstore.put('chunk{}/image'.format(i_chunk),pd.DataFrame(\n",
    "        img_df['image'].apply(lambda x: np.ravel(x)).values\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/matt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2862: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->[0]]\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 of 193 complete\n",
      "Chunk 2 of 193 complete\n",
      "Chunk 3 of 193 complete\n",
      "Chunk 4 of 193 complete\n",
      "Chunk 5 of 193 complete\n",
      "Chunk 6 of 193 complete\n",
      "Chunk 7 of 193 complete\n",
      "Chunk 8 of 193 complete\n",
      "Chunk 9 of 193 complete\n",
      "Chunk 10 of 193 complete\n",
      "Chunk 11 of 193 complete\n",
      "Chunk 12 of 193 complete\n",
      "Chunk 13 of 193 complete\n",
      "Chunk 14 of 193 complete\n",
      "Chunk 15 of 193 complete\n",
      "Chunk 16 of 193 complete\n",
      "Chunk 17 of 193 complete\n",
      "Chunk 18 of 193 complete\n",
      "Chunk 19 of 193 complete\n",
      "Chunk 20 of 193 complete\n",
      "Chunk 21 of 193 complete\n",
      "Chunk 22 of 193 complete\n",
      "Chunk 23 of 193 complete\n",
      "Chunk 24 of 193 complete\n",
      "Chunk 25 of 193 complete\n",
      "Chunk 26 of 193 complete\n",
      "Chunk 27 of 193 complete\n",
      "Chunk 28 of 193 complete\n",
      "Chunk 29 of 193 complete\n",
      "Chunk 30 of 193 complete\n",
      "Chunk 31 of 193 complete\n",
      "Chunk 32 of 193 complete\n",
      "Chunk 33 of 193 complete\n",
      "Chunk 34 of 193 complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-69d96842b62e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mi_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_df\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_to_category\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mstore_to_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_img_hdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#        img_df.to_hdf(proc_img_hdf,'image/chunk{}'.format(i_chunk))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-651c2772bdcb>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-651c2772bdcb>\u001b[0m in \u001b[0;36mscale_images\u001b[0;34m(image_series)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrescaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_dim_224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrescaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-651c2772bdcb>\u001b[0m in \u001b[0;36mfirst_dim_224\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m\"\"\"Scale first dimension of image to 224 pixels\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mrescaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_dim_224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrescaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mrescale\u001b[0;34m(image, scale, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     return resize(image, output_shape, order=order, mode=mode, cval=cval,\n\u001b[0;32m--> 205\u001b[0;31m                   clip=clip, preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    133\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    134\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    773\u001b[0m                     dims.append(_warp_fast(image[..., dim], matrix,\n\u001b[1;32m    774\u001b[0m                                            \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                            order=order, mode=mode, cval=cval))\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mwarped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2637)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image data\n",
    "# Broke at chunksize 500 at chunk 197\n",
    "chunksize = 1500 # Field becomes too large to store if this goes much higher\n",
    "\n",
    "# Clear the stored file if it already exists\n",
    "if os.path.exists(proc_img_path):\n",
    "    os.remove(proc_img_path)\n",
    "\n",
    "start = perf_counter(); image_count = 0\n",
    "with pd.HDFStore(proc_img_path,'a',complevel=9) as proc_img_hdf:\n",
    "    i_chunk = 0\n",
    "    for n_chunk, img_df in load_data(image_to_category,chunksize):\n",
    "        img_df = preprocess_data(img_df)\n",
    "        store_to_hdf5(proc_img_hdf,img_df,i_chunk)\n",
    "#        img_df.to_hdf(proc_img_hdf,'image/chunk{}'.format(i_chunk))\n",
    "\n",
    "        image_count += len(img_df)\n",
    "        i_chunk += 1\n",
    "        print('Chunk {} of {} complete'.format(i_chunk,n_chunk))\n",
    "    proc_img_hdf.get_node('image')._f_setattr('chunksize',chunksize)\n",
    "print('Took {} seconds to preprocess {} images'.format(perf_counter() - start,image_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAENCAYAAADkNanAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB0lSh+EgSer4/3j8xffIkKYaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92db347320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Categories by number of occurences in img_df\n",
    "labels = determine_data_subset(image_to_category)\n",
    "labels.groupby('category_label')['image_name'].count().sort_values(ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows images from the last images to be processed\n",
    "imshow(img_df['image'].iloc[random.randint(0,img_df.shape[0]-1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the ImageNet models we'll use as  a baseline\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "# define ResNet50 model\n",
    "res50 = ResNet50(weights='imagenet')\n",
    "#vgg19 = VGG19(weights='imagenet')\n",
    "incv3 = InceptionV3(weights='imagenet')\n",
    "xcept = Xception(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepFashion dataset assigns one clothing category to each image, even if more articles of clothing are visible. To account for this, when predicting we take the most likely category as the sole image category. \n",
    "\n",
    "When comparing ImageNet predictions to DeepFashion predictions, category names are matched by hand. However, not all categories have a reasonable counterpart. For this reason we calculate accuracy considering only those categories that can be matched. More explicitly: \n",
    "- We exclude all images from the DeepFashion dataset that do not have at least one ImageNet category corresponding to their true category\n",
    "- When calculating the most probable ImageNet category, we only take the max probability across ImageNet categories that can be matched to a DeepFashion category\n",
    "- A prediction is always made - no matter how low the category probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate some statistics on how ImageNet models with no tuning perform on the DeepFashion dataset\n",
    "model_dict = {\n",
    "        'resnet' : res50,\n",
    "#        'vgg19' : vgg19,\n",
    "        'inception_v3' : incv3,\n",
    "        'xception' : xcept,\n",
    "    }\n",
    "\n",
    "def generate_baseline_stats(img_df):\n",
    "    \"\"\"\n",
    "    Generates a report on the baseline performance of a pre-trained ImageNet classifier\n",
    "    \"\"\"\n",
    "    # Map of the ImageNet labels to DeepFashion labels\n",
    "    # Determined by crossreferencing:\n",
    "    #     https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "    # with the DeepFashion category labels which match the (1 based) index as listed in: \n",
    "    #      data/Category and Attribute Prediction Benchmark/Anno/list_category_cloth\n",
    "    # Top 10 IDs: 41, 18, 3, 32, 17, 33, 6, 16, 11, 19\n",
    "    # The associations are not perfect, they will be evaluated individually later\n",
    "    img_net_to_deep_fash_names = {\n",
    "        'overskirt' : ['Dress'], # ?\n",
    "        'hoopskirt, crinoline' : ['Dress'], # ?\n",
    "        'jersey, T-shirt, tee shirt' : ['Tee','Blouse'], # OK, ?\n",
    "#        : 'Shorts',\n",
    "#        : 'Tank',\n",
    "        'miniskirt, mini' : ['Skirt'], # OK\n",
    "#        : 'Cardigan',\n",
    "        'sweatshirt' : ['Sweater'], # ?\n",
    "#        : 'Jacket',\n",
    "#        : 'Top', # Too diverse - may want to drop entirely\n",
    "    }\n",
    "    img_net_to_deep_fash_id = build_id_map(img_net_to_deep_fash_names)\n",
    "    \n",
    "    # Only predict on images with a category ImageNet models can predict\n",
    "    valid_ids = [ id_ + 1 for id_list in img_net_to_deep_fash_id.values() for id_ in id_list ]\n",
    "    relevant_img_df = img_df.loc[img_df['category_label'].isin(valid_ids),:]\n",
    "\n",
    "\n",
    "    # Get the prediction accuracy with each of the models in model_dict below\n",
    "    image_net_predict(img_net_to_deep_fash_id,model_dict,relevant_img_df)\n",
    "    acc_dict = calc_accuracy(img_net_to_deep_fash_id,model_dict,relevant_img_df)\n",
    "    return acc_dict\n",
    "    \n",
    "def image_net_predict(id_map,model_dict,img_df):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, model in model_dict.items():\n",
    "        start = perf_counter()\n",
    "        preds = model.predict(np.stack(img_df['image'].values))\n",
    "        print('{} took {} seconds to predict {} images'.format(name,perf_counter() - start,len(preds)))\n",
    "        img_df[name] = get_img_net_cat(preds,id_map)\n",
    "\n",
    "def get_img_net_cat(preds,id_map):\n",
    "    # Get just the predictions for categories that map to DeepFashion categories\n",
    "    valid_cat_ids = list(id_map.keys())\n",
    "    preds = preds[:,valid_cat_ids]\n",
    "    \n",
    "    # Get the most probable index as indexed in the full ImageNet category list\n",
    "    max_indices = np.argmax(preds,axis=1)\n",
    "    max_indices = [ valid_cat_ids[ind] for ind in max_indices ]\n",
    "    return max_indices\n",
    "\n",
    "def calc_accuracy(id_map,model_dict,img_df):\n",
    "    def get_outcome(model_name,row):\n",
    "        pred = id_map[row[model_name]]\n",
    "        if row['category_label'] - 1 in pred:\n",
    "            return 1\n",
    "        return 0\n",
    "    acc_dict = {}\n",
    "    for name in model_dict.keys():\n",
    "        acc_dict[name] = sum(img_df.apply(partial(get_outcome,name),axis=1))/img_df.shape[0]\n",
    "    return acc_dict\n",
    "\n",
    "def build_id_map(name_map):\n",
    "    # Get ImageNet ID to name map. IDs start at 0\n",
    "    with open(img_net_class_path) as class_def:\n",
    "        img_net_dict = eval(''.join(list(class_def)))\n",
    "    \n",
    "    # Get DeepFashion ID to name map. IDs start at 0\n",
    "    deep_fash_dict = pd.read_csv(deep_fash_class_path,\n",
    "                    skiprows=1,sep='\\s+',usecols=['category_name']).to_dict()['category_name']\n",
    "    \n",
    "    # Reverse the dictionaries\n",
    "    img_net_dict = { val : key for key, val in img_net_dict.items() }\n",
    "    deep_fash_dict = { val : key for key, val in deep_fash_dict.items() }\n",
    "\n",
    "    net_id_to_fash_id = {}\n",
    "    for k,v in name_map.items():\n",
    "        for name in v:\n",
    "            try:\n",
    "                net_id_to_fash_id[img_net_dict[k]].append(deep_fash_dict[name])\n",
    "            except (KeyError,AttributeError):\n",
    "                net_id_to_fash_id[img_net_dict[k]] = [deep_fash_dict[name]]\n",
    "    return net_id_to_fash_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_baseline_stats(img_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results above, I think my chosen mapping may not be that accurate. The chosen models have well over 80% accuracy on the ImageNet dataset. Numbers as low as those reported above signals a problem with the data, likely the choice of corresponding classes is not the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottleneck Feature Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_frozen_layer(base_model,unfrozen_layers):\n",
    "    \"\"\"\n",
    "    Returns the layer just before the first unfrozen layer, the output node index \n",
    "    and inbound node index for that layer. \n",
    "    Unfrozen layers only counts layers with trainable weights. \n",
    "    base_model - The model with layers we want to freeze\n",
    "    unfrozen_layers - The number of trainable layers to skip before the target layer\n",
    "    \"\"\"\n",
    "    unfrozen_count = 0 # -1 because we want one more than the unfrozen_layers\n",
    "    # Get the name of the first layer to be left frozen\n",
    "    for l_num in range(len(base_model.layers_by_depth)):\n",
    "        if unfrozen_count == unfrozen_layers:\n",
    "            break\n",
    "        for layer in base_model.layers_by_depth[l_num]:\n",
    "            if layer.weights:\n",
    "                unfrozen_count += 1\n",
    "                if unfrozen_count == unfrozen_layers:\n",
    "                    target_index = get_target_index(l_num,base_model.layers_by_depth)\n",
    "                    in_node, out_node = get_node_indices(target_index,base_model.layers_by_depth)\n",
    "                    break\n",
    "        \n",
    "    try:\n",
    "        return target_index, in_node, out_node\n",
    "    except NameError:\n",
    "        print('Error: All layers or more layers than the network has left unfrozen')\n",
    "        raise(AssertionError)\n",
    "        \n",
    "def get_target_index(l_num,layer_dict):\n",
    "    \"\"\"Gets the layer index before the layer at l_num\"\"\"\n",
    "    l_num += 1\n",
    "    # Avoid multiple layer entries, complicates implementation\n",
    "    while len(layer_dict[l_num]) != 1:\n",
    "        l_num += 1\n",
    "    return l_num\n",
    "        \n",
    "def get_node_indices(layer_key, layer_dict):\n",
    "    \"\"\"Making sure we get the right input and output nodes for the layer\"\"\"\n",
    "    # Will complete implementation if necessary. Will be difficult to handle edge cases right\n",
    "    return -1, 0\n",
    "\n",
    "    target_layer = layer_dict[layer_key]\n",
    "    if len(target_layer.outbound_nodes) == 1:\n",
    "        out_ind = 0\n",
    "    else:\n",
    "        # Scan to end of network for first matching node\n",
    "        out_ind = -1\n",
    "        cur_ind = layer_key - 1\n",
    "        while cur_ind >= 0:\n",
    "            for layer in layer_dict[cur_ind]:\n",
    "                for node in layer.inbound_nodes:\n",
    "                    if node in target_layer.outbound_nodes:\n",
    "                        out_ind = target_layer.outbound_nodes.index(node)\n",
    "            if out_ind != -1:\n",
    "                break\n",
    "            cur_ind -= 1\n",
    "    \n",
    "    if len(target_layer.inbound_nodes) == 1:\n",
    "        in_ind = 0\n",
    "    else:\n",
    "        # Scan to end of network for first matching node\n",
    "        in_ind = -1\n",
    "        cur_ind = layer_key + 1\n",
    "        while cur_ind < len(layer_dict):\n",
    "            for layer in layer_dict[cur_ind]:\n",
    "                for node in layer.outbound_nodes:\n",
    "                    if node in target_layer.inbound_nodes:\n",
    "                        in_ind = target_layer.inbound_nodes.index(node)\n",
    "            if in_ind != -1:\n",
    "                return in_ind, out_ind\n",
    "            cur_ind += 1\n",
    "        in_ind = 0\n",
    "    return in_ind, out_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate bottleneck features and store them\n",
    "img_df = pd.read_hdf('intermediates/processed_images.hdf5','images')\n",
    "\n",
    "with pd.HDFStore('intermediates/bottleneck_feats.hdf5','w') as feat_store:\n",
    "    feat_store.append('labels',img_df[['image_name','category_label']])\n",
    "    for name, model in model_dict.items():\n",
    "        # A max of 5 trainable layers back from output will be trained\n",
    "        out_index, in_node, out_node = first_frozen_layer(model,5)\n",
    "        bottleneck_model = Model(inputs=model.input,outputs=model.layers_by_depth[out_index][0].get_output_at(out_node))\n",
    "        \n",
    "        # Break into batches over images here if necessary\n",
    "        start = perf_counter()\n",
    "        bottleneck_feats = bottleneck_model.predict(np.stack(img_df['image'].values))\n",
    "        print('{} took {} seconds to generate {} feature sets'.format(name,perf_counter() - start,len(bottleneck_feats)))\n",
    "        # all are 4D arrays, will need to find a more robust storage method if that changes\n",
    "        feat_store.append('feat_data/'+name,pd.Panel4D(bottleneck_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a model tuned to the DeepFashion dataset will be trained. The considered hyperparameters will\n",
    "be:\n",
    "- Transfer learning base: Resnet, Inception V3, Xception\n",
    "- Layer at which to unfreeze weights\n",
    "- Epoch to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_model_until_layer(base_model,first_layer,]):\n",
    "    \"\"\"\n",
    "    Copies base_model's architecture from output to the given index.\n",
    "    \n",
    "    Adds a new custom output\"\"\"\n",
    "model = Sequential()\n",
    "model.add(InputLayer((7, 7, 512)))\n",
    "layer1 = res50.layers_by_depth[6][0]\n",
    "model.add(layer1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froze_layer = first_frozen_layer(res50,2)\n",
    "model = Model(inputs=res50.input, outputs=froze_layer.get_output_at())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((1,23,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers_by_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50.layers_by_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50.get_layer('bn3b_branch2c').inbound_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1.get_output_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('input_13').output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1.inbound_nodes.index(layer1.inbound_nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = model.predict(np.random.random((3, 7, 7, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'input',\n",
    " 'input_layers',\n",
    " 'input_layers_node_indices',\n",
    " 'input_layers_tensor_indices',\n",
    " 'input_mask',\n",
    " 'input_names',\n",
    " 'input_shape',\n",
    " 'input_spec',\n",
    " 'inputs',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Search over hyperparameters for the best model\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import InputLayer\n",
    "np.random.seed(313) # For reproducibility\n",
    "\n",
    "def stratified_train_val_test(X,y):\n",
    "    \"\"\"Generate stratified train-val-test splits. Hard coded for 70-20-10 proportion\"\"\"\n",
    "    # Train-test split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1,test_size=.1)\n",
    "    train, test = next(sss.split(X,y))\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # Train-validate split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1,test_size=2/9)\n",
    "    train, valid = next(sss.split(X_train,y_train))\n",
    "    X_valid, y_valid = X_train[valid], y_train[valid]\n",
    "    X_train, y_train = X_train[train], y_train[train]\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def generate_bottleneck_features(base_model,max_unfrozen_layers,input_data):\n",
    "    output_layer, in_node_ind, out_node_ind = first_frozen_layer(base_model,max_unfrozen_layers)\n",
    "    model = Model(inputs=base_model.input, outputs=output_layer.get_output_at(out_node_ind))\n",
    "    model.compile()\n",
    "    model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import clone_model\n",
    "\n",
    "clone_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(base_net,unfrozen_layers):\n",
    "    \"\"\"\n",
    "    base_net - pretrained ImageNet classifier\n",
    "    unfrozen_layers - number of layers to leave trainable in base_net\n",
    "    \"\"\"\n",
    "    base_model = base_net(weights='imagenet')\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer(first_frozen).output)\n",
    "            \n",
    "    model\n",
    "            \n",
    "def create_pretrained_embedding_model(top_words,max_review_length,embedding_matrix):\n",
    "    embedding_vector_length = 300\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(top_words,\n",
    "                             embedding_vector_length,\n",
    "                             weights=[embedding_matrix],\n",
    "                             input_length=max_review_length,\n",
    "                             trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(100)) # LSTM layer, top_words x embedding_vector_length -> 100 dim \n",
    "    model.add(Dense(5, activation='softmax')) # The output node, 100 -> 5 dim\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer(first_frozen).output)\n",
    "    model\n",
    "\n",
    "    return model\n",
    "# checkpointer = ModelCheckpoint(filepath='saved_models/best_weights_{model}_{unfreeze}.hdf5', \n",
    "#                                verbose=1, save_best_only=True)\n",
    "# \n",
    "# VGG16_model.fit(train_VGG16, train_targets, \n",
    "#           validation_data=(valid_VGG16, valid_targets),\n",
    "#           epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/best_weights_{model}_{unfreeze}.hdf5', \n",
    "                                verbose=1, save_best_only=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=partial(build_model,base_net,unfrozen_layers),callbacks=[checkpointer],verbose=1)\n",
    "# # grid search epochs, batch size and optimizer\n",
    "# optimizers = ['rmsprop', 'adam']\n",
    "# init = ['glorot_uniform', 'normal', 'uniform']\n",
    "# epochs = [50, 100, 150]\n",
    "# batches = [5, 10, 20]\n",
    "# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "# grid_result = grid.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
